{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST736 Text Mining - Homework 2\n",
    "## Martin Alonso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "In this assignment, you will vectorize the data that you collected in HW1. Because the goal is to\n",
    "identify the public sentiment toward AI on social media, you need to think about what\n",
    "vectorization options, regarding both what to count and how to count, would be the best for this\n",
    "goal. Make sure to explain the decisions you made during the vectorization process, e.g., if you\n",
    "removed stopwords and why.\n",
    "\n",
    "Write a report to include the following information:  \n",
    "(1) Briefly recap how you collected the data.  \n",
    "(2) Describe your vectorization choices and corresponding result. For example, if you chose to do stemming, how did the vocabulary size change after stemming? Did the stemming eliminate important linguistic information that you'd rather keep, or not?  \n",
    "(3) Conclude with the best vectorization option(s).  \n",
    "Your report should provide sufficient information for others to replicate what you did. Submit your report with your original data file and the vectors from your best vectorization options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "(1) For my previous homework, I used the `twitterscrape` package to collect 200 tweets from Twitter. The library takes as inputs the word (or words) that the user wants to search for, the language, number of tweets to collect, and format of the final file. For my data, I chose to collect the latest 200 tweets that featured either the words \"Artificial Intelligence\" or \"AI\". I also made sure that the language was set to English, so as to avoid tweets in French that featured the first person singular conjugation of the verb \"etre\" (i.e.: \"j'ai\"). Once the tweets were collected, I exported the file as a comma-separated value file. \n",
    "The exported data set featured the following columns: \n",
    "* user: Twitter handle\n",
    "* id: tweet id\n",
    "* timestamp: date-time tweet was sent\n",
    "* fullname: user name\n",
    "* text: tweet  \n",
    "\n",
    "I replaced the id column, dropped the fullname columns, and added a 'label' column that flagged whether the overall sentiment of the tweet was positive (2), neutral (1), or negative (0). This last column was filled after reading every tweet, which shows my personal bias towards whether I found the tweet to be positive, negative, or neutral. \n",
    "Having prepared the data, I loaded it into my workspace and began my sentiment analysis on it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for data upload, preparation, and vectorization\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer words in dictionary: 1279\n",
      "TF-IDF Vectorizer words in dictionary: 1279\n"
     ]
    }
   ],
   "source": [
    "# Load the data set \n",
    "tweets = pd.read_csv('ai_tweets.csv')\n",
    "tweets.head()\n",
    "\n",
    "# Isolate the tweets in a corpus list\n",
    "corpus = tweets['text']\n",
    "\n",
    "# Create a count vectorizer that drops any word that contains a number in it, since these words are likely part of the tweet URL and not likely to \n",
    "# help in finding most common words.\n",
    "cv = CountVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b')\n",
    "Xcv = cv.fit_transform(corpus)\n",
    "\n",
    "# Create a TF-IDF vectorizer to compare results. \n",
    "tfidf = TfidfVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b')\n",
    "Xti = tfidf.fit_transform(corpus)\n",
    "\n",
    "# Print length of the dictionaries to serve as a baseline. \n",
    "print(\"Count Vectorizer words in dictionary: %s\" % len(cv.get_feature_names()))\n",
    "print('TF-IDF Vectorizer words in dictionary: %s' % len(tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer without stop words: 1136\n",
      "TF-IDF Vectorizer without stop words: 1136\n"
     ]
    }
   ],
   "source": [
    "# By removing numbers, and words that have numbers within, we've managed to produce two clean dictionaries that features 1,279 words. So far there \n",
    "# are no differences between the two vectorizers. \n",
    "# Now let's remove both numbers, words with numbers, and English stop words.  \n",
    "\n",
    "# Second count vectorizer\n",
    "cv2 = CountVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b', stop_words='english')\n",
    "Xcv2 = cv2.fit_transform(corpus)\n",
    "\n",
    "# Second tf-idf vectorizer\n",
    "tfidf2 = TfidfVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b', stop_words='english')\n",
    "Xti2 = tfidf2.fit_transform(corpus)\n",
    "\n",
    "# Dictionary length. \n",
    "print('Count Vectorizer without stop words: %s' % len(cv2.get_feature_names()))\n",
    "print('TF-IDF Vectorizer without stop words: %s' % len(tfidf2.get_feature_names()))\n",
    "\n",
    "# No change whatsoever; both vectorizers have removed the same number of words (143) when removing stop words from the original dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Count Vectorizer: 1028\n",
      "Stemmed TF-IDF Vectorizer: 1028\n"
     ]
    }
   ],
   "source": [
    "# Having removed suspect words and stop words, we'll try stemming as a way to reduce the dictionary. \n",
    "# Loads the English-word stemmer from the NLTK package\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "# Builds Count and TF-IDF analyzers that consider patterns and stop words. \n",
    "cv3 = CountVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b', stop_words='english').build_analyzer()\n",
    "tfidf3 = TfidfVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b', stop_words='english').build_analyzer()\n",
    "\n",
    "# Function that stems every word in the corpus. Loops over every word to find the stemmed word for faster computation\n",
    "def stemmed_cv_words(doc):\n",
    "    return (stemmer.stem(w) for w in cv3(doc))\n",
    "\n",
    "def stemmed_tfidf_words(doc):\n",
    "    return (stemmer.stem(w) for w in tfidf3(doc))\n",
    "\n",
    "# Final vectorizer that outputs dictionary lengths\n",
    "cv_stem = CountVectorizer(analyzer=stemmed_cv_words)\n",
    "cv3 = cv_stem.fit_transform(corpus)\n",
    "\n",
    "tfidf_stem = TfidfVectorizer(analyzer=stemmed_tfidf_words)\n",
    "tfidf3 = tfidf_stem.fit_transform(corpus)\n",
    "\n",
    "print('Stemmed Count Vectorizer: %s' % len(cv_stem.get_feature_names()))\n",
    "print('Stemmed TF-IDF Vectorizer: %s' % len(tfidf_stem.get_feature_names()))\n",
    "\n",
    "# Again, both dictionaries have reduced the same number of words, dropping 108 words for a final tally of 1,028 words in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hovewer, we should check the most common words in each vectorizer. Perhaps this will give us a clue as to whether the vectorizers are acting similarly.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gets the labels and sum of words for Count Vectorizer. Then, we convert to Pandas series and Data Frame to order data by most common words. \n",
    "# Finally, we plot the most common words to see how the vectorizer has stemmed the words. \n",
    "cv_labels, cv_values = cv_stem.get_feature_names(), cv3.sum(axis=1)\n",
    "cv_labels = pd.Series(cv_labels)\n",
    "cv_values = pd.DataFrame(cv_values)\n",
    "cv_count = pd.concat([cv_labels, cv_values], axis=1, ignore_index=True)\n",
    "cv_count.columns = ['labels', 'values']\n",
    "cv_count = cv_count.sort_values(by=['values'], ascending=False)\n",
    "\n",
    "# Repeat the process for the TF-IDF Vectorizer. \n",
    "tfidf_labels, tfidf_values = tfidf_stem.get_feature_names(), tfidf3.sum(axis=1)\n",
    "tfidf_labels = pd.Series(tfidf_labels)\n",
    "tfidf_values = pd.DataFrame(tfidf_values)\n",
    "tfidf_count = pd.concat([tfidf_labels, tfidf_values], axis=1, ignore_index=True)\n",
    "tfidf_count.columns = ['labels', 'values']\n",
    "tfidf_count = tfidf_count.sort_values(by=['values'], ascending=False)\n",
    "\n",
    "# Select the top ten most common words to verify if the vectorizers are splitting the same way. \n",
    "cv_count_top10 = cv_count[:10]\n",
    "tfidf_count_top10 = tfidf_count[:10]\n",
    "\n",
    "# Plot the vectorizers for visual comparison. \n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='labels', y='values', data=cv_count_top10)\n",
    "_ = plt.xlabel('Words')\n",
    "_ = plt.xticks(rotation=15)\n",
    "_ = plt.ylabel('Count')\n",
    "_ = plt.title('Count Vectorizer: Most Common Words')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x='labels', y='values', data=tfidf_count_top10)\n",
    "_ = plt.xlabel('Words')\n",
    "_ = plt.xticks(rotation=15)\n",
    "_ = plt.ylabel('Count')\n",
    "_ = plt.title('TF-IDF Vectorizer: Most Common Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, despite featuring the same number of words, both vectorizers do not count the frequencies of these the same way. There are some words in common (i.e. 'age', 'artificia', 'app', 'attend', 'centuri' are all featured in both vectorizers top 10) but the term frequency is calculated differently. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Min Term 2: 199\n",
      "TF-IDF Vectorizer Min Term 2: 187\n"
     ]
    }
   ],
   "source": [
    "# As an additional exercise, I'll subject both dictionaries to a minimum term frequency of two, gauging if \n",
    "# there is a significant drop in the number of terms. \n",
    "cv_count2 = cv_count[cv_count['values'] >= 2]\n",
    "tfidf_count2 = tfidf_count[tfidf_count['values'] >= 2]\n",
    "\n",
    "print('Count Vectorizer Min Term 2: %s' % len(cv_count2))\n",
    "print('TF-IDF Vectorizer Min Term 2: %s' % len(tfidf_count2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a very significant drop in the number of terms featured in each vectorizer. The Count vectorizer drops 829 terms that appear only once in the corpus, while the TF-IDF vectorizer drops 841 terms.  \n",
    "Judging by the graphs, TF-IDF is more restrictive as it is weighing the terms based on the rest of the term frequencies in the corpus, making it much more stringent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "(3) After reviewing all the process that created both dictionaries, it is hard to tell at first glance which is the preferred vectorizer of the two of them. They both have their points in favor and against. The Count vectorizer manages to count all instances of a term appearing in the corpus, which is preferred if we're trying to identify trends in the language. However, the TF-IDF vectorizer weights the amount of times a term appears within the corpus against the rest of the terms in it, ranking the terms by their relative importance.   \n",
    "If we want to know which words are the most common, then we may as well stick with the Count Vectorizer. But, if our goal is to study the relative importance of a term with relation to the terms around it, then TF-IDF may prove to be the superior vectorizer for the task. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
