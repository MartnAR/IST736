{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sentiment analysis and topic modeling \n",
    "* Correlation between topic and sentiment\n",
    "\n",
    "1. what are you looking for\n",
    "2. why does it matter\n",
    "3. how do you know you found what you were looking for\n",
    "\n",
    "https://developer.twitter.com/en/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import json, codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,12)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files obtained through twitterscraper \n",
    "with codecs.open('harper_tweets.json', 'r', 'utf-8') as harper_tweets:\n",
    "    harper_tweets = json.load(harper_tweets, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract each element from the tweets and convert them to a pandas DataFrame \n",
    "list_tweets = [list(elem.values()) for elem in harper_tweets]\n",
    "list_columns = list(harper_tweets[0].keys())\n",
    "df = pd.DataFrame(list_tweets, columns=list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('harper_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these tweets don't have any location information, we'll need to use the `tweepy` package and have access to the Twitter developers API to extract the location from each tweet id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tweepy package\n",
    "import tweepy \n",
    "\n",
    "# Consumer and access keys that allow access to the Twitter developer API \n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "# Create the tweepy API \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load TweetLocation.py\n",
    "import tweepy \n",
    "import pandas as pd \n",
    "\n",
    "class tweet_location:\n",
    "    \"\"\"\n",
    "    Obtains tweet location by passing tweet ids through the parser.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "\n",
    "    def get_location(id):\n",
    "        try:\n",
    "            tweet_status = api.get_status(id)\n",
    "            tweet_location = tweet_status.user.location\n",
    "            tweet_data = [id, tweet_location]\n",
    "            return(tweet_data)\n",
    "        except:\n",
    "            print(\"Tweet ID not found\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running chunk 1 of 250\n",
      "... chunk start: 0\n",
      "... chunk end: 1161\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tweet_ids = df['id']\n",
    "    tweet_loc = []\n",
    "    \n",
    "    exec_start_time = time.time()\n",
    "    \n",
    "    with Pool(cpu_count()) as p:\n",
    "    \n",
    "        chunks = 250\n",
    "        chunk_size = round(0.5 + len(tweet_ids)/chunks)\n",
    "    \n",
    "        for chunk_i in np.arange(0, chunks):\n",
    "            print(\"Running chunk {} of {}\".format(chunk_i + 1, chunks))\n",
    "            chunk_start = chunk_i * chunk_size\n",
    "            chunk_end = chunk_start + chunk_size\n",
    "            chunk_end = min(len(tweet_ids), chunk_end)\n",
    "            print(\"... chunk start: {}\".format(chunk_start))\n",
    "            print(\"... chunk end: {}\".format(chunk_end))\n",
    "\n",
    "            tmp = p.map(tweet_location.get_location, list(tweet_ids)[chunk_start:chunk_end])\n",
    "            for i in tmp:\n",
    "                tweet_location.append(i)\n",
    "\n",
    "print(\"Loop took {:.2f} minutes\".format((time.time() - exec_start_time)/60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
