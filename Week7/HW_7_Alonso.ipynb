{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST736 Text Mining\n",
    "## Homework 7\n",
    "### Martin Alonso\n",
    "### 2019-03-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "For this assignment, there are three tasks that will be completed using Sentiment Analysis data from [Kaggle](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data). \n",
    "1. Build a MultinomialNB and SVM model using unigram features. Compare the top ten words for both positive negative categories. Report the confusion matrix, recall, and precision for both models. Both models must have the same parameter features. \n",
    "2. Build a second MultinomialNB and SVM model using CountVectorizers. Split the training set 60/40, keeping the original parameters from task 1, except this time build a bigram model. Again build a confusion matrix and compare recall and precision of both models. \n",
    "3. Revise the model script and build an SVM using the full training set. Tune the model to gain maximum possible accuracy, reporting parameter tuning and cross validation accuracy. Use this model to predict both the test set of the Kaggle data and submit a prediction to the Kaggle competition. \n",
    "\n",
    "### Analysis\n",
    "We'll start by first importing the necessary libraries, specifically `pandas` and `sklearn`. We'll then load the data and do some quick exploration before diving into the individual tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load librares\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data set and view first five rows\n",
    "df = pd.read_csv('C:/Users/malon/Documents/Syracuse University/IST 736 Text Mining/IST736/IST736/Week6/kaggle-sentiment/kaggle-sentiment/train.tsv', delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4)\n",
      "Sentiment\n",
      "0     7072\n",
      "1    27273\n",
      "2    79582\n",
      "3    32927\n",
      "4     9206\n",
      "Name: SentenceId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show shape of data set and count number of items per sentiment\n",
    "print(df.shape)\n",
    "\n",
    "num_items = df.groupby('Sentiment')['SentenceId'].count()\n",
    "print(num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 156,060 observations, split up among five different sentiment classifications, ranging from very negative (0) to very positive (4).  \n",
    "We'll now identify the X and y columns which will be the Phrase and Sentiment columns; then we'll start with the tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Multinomial Naive Bayes\n",
    "We'll first start by taking all words in the data set, lowering, and stemming them with the Snowball Stemmer from the `nltk` package. After, we'll separate the data into training and test sets. Once this is done, a TF-IDF vectorizer will be built using unigram words, latin-encoding and removing stop words. \n",
    "With the model built and trained, we'll test the results on the remaining 40 percent of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame in case the data needs to be reloaded\n",
    "dat = df\n",
    "\n",
    "# Remove special character, set the dataset to lower case\n",
    "for i in range(0, len(dat)): \n",
    "    dat.loc[i, 'Phrase'] = re.sub('[\\W\\_]', ' ', dat.loc[i, 'Phrase']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Phrase  \\\n",
      "0  a series of escapades demonstrating the adage ...   \n",
      "1  a series of escapades demonstrating the adage ...   \n",
      "2                                           a series   \n",
      "3                                                  a   \n",
      "4                                             series   \n",
      "\n",
      "                                      stemmed_review  \n",
      "0  a series of escapades demonstrating the adage ...  \n",
      "1  a series of escapades demonstrating the adage ...  \n",
      "2                                             a seri  \n",
      "3                                                  a  \n",
      "4                                               seri  \n"
     ]
    }
   ],
   "source": [
    "# Initiate English stemmer for data set\n",
    "englishStemmer=SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "for i in range(0, len(dat)): \n",
    "    dat.loc[i, 'stemmed_review'] = englishStemmer.stem(dat.loc[i, 'Phrase'])\n",
    "    \n",
    "# Review first five observations. \n",
    "print(dat[['Phrase', 'stemmed_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been transformed, we'll separate it into training and test sets and build the Multinomial Naive Bayes model using a TF-IDF Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify X and y columns\n",
    "X = dat['stemmed_review'].values\n",
    "y = dat['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Build the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate the Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Fit transform the training set with the tf_idf vectorizer\n",
    "X_train_mnb_task1 = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Fit the model on the data \n",
    "mnb.fit(X_train_mnb_task1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   79  1138  1539    69     0]\n",
      " [   29  2616  7901   377     2]\n",
      " [    7  1078 28723  1774    15]\n",
      " [    0   166  8190  4898    65]\n",
      " [    0    21  1414  2179   144]]\n",
      "The model has a 58.0 percent accuracy score\n",
      "The model has a precision score of 0.5774402400505637\n",
      "The model has a recall score of 0.5840702293989491\n"
     ]
    }
   ],
   "source": [
    "# Transform X_test using tfidf_vectorizer\n",
    "X_test_mnb_task1 = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Run the model against the test set\n",
    "mnb_task1_pred = mnb.predict(X_test_mnb_task1)\n",
    "\n",
    "# Compare the predictions with the actual outcomes; print a confusion matrix\n",
    "mnb_task1_results = confusion_matrix(y_test, mnb_task1_pred)\n",
    "\n",
    "mnb_task1_matrix = []\n",
    "for i in range(0, 5):\n",
    "        mnb_task1_matrix.append(mnb_task1_results[i,i])\n",
    " \n",
    "print(mnb_task1_results)\n",
    "print('The model has a {} percent accuracy score'.format(np.round(np.sum(mnb_task1_matrix)/len(X_test) * 100), 3))\n",
    "print(\"The model has a precision score of {}\".format(precision_score(y_test, mnb_task1_pred, average='weighted')))\n",
    "print(\"The model has a recall score of {}\".format(recall_score(y_test, mnb_task1_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the top ten most influencing words for both very positive and very negative scores are the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top ten most negative words: mnb_task1_neg\n",
      "[(-6.9399772891086, 'plot'), (-6.920909330556597, 'does'), (-6.916481813146056, 'long'), (-6.614610163989215, 'movi'), (-6.469718645676306, 'just'), (-6.4612640564650885, 'worst'), (-6.36007690841749, 'like'), (-5.9943176509807214, 'film'), (-5.657573643770104, 'movie'), (-5.487203932578288, 'bad')]\n",
      "\n",
      " The top ten most positive words: \n",
      "[(-6.426636412219698, 'movi'), (-6.374529025683264, 'fun'), (-6.334336216735293, 'year'), (-6.290101220903835, 'perform'), (-6.257090422266336, 'great'), (-6.125897821477724, 'movie'), (-6.091946010331331, 'good'), (-6.05447637421946, 'funny'), (-5.667597087531533, 'best'), (-5.303799225452078, 'film')]\n"
     ]
    }
   ],
   "source": [
    "# Print top 10 very positive and very negative words.\n",
    "mnb_task1_neg = sorted(zip(mnb.coef_[0], tfidf_vectorizer.get_feature_names()))\n",
    "mnb_task1_very_negative_features = mnb_task1_neg[-10:]\n",
    "print(\"The top ten most negative words: mnb_task1_neg\")\n",
    "print(mnb_task1_very_negative_features)\n",
    "\n",
    "mnb_task1_pos = sorted(zip(mnb.coef_[4], tfidf_vectorizer.get_feature_names()))\n",
    "very_positive_features = mnb_task1_pos[-10:]\n",
    "print(\"\\r\\n The top ten most positive words: \")\n",
    "print(very_positive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine\n",
    "Now let's compare the results from the Multinomial Naive Bayes to those of the SVM. Given that the data has already been prepared, we'll skip ahead to building the model and reporting the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate the Support Vector Classifier\n",
    "svm = LinearSVC()\n",
    "\n",
    "# Fit transform the training set with the tf_idf vectorizer\n",
    "X_train_svm_task1 = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Fit the model on the data \n",
    "svm.fit(X_train_svm_task1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  752  1424   545    98     6]\n",
      " [  624  4498  5182   582    39]\n",
      " [  176  2381 26305  2598   137]\n",
      " [   28   445  5674  6296   876]\n",
      " [    5    57   544  2013  1139]]\n",
      "The model has a 62.0 percent accuracy score\n",
      "The model has a precision score of 0.6061185913896154\n",
      "The model has a recall score of 0.6245995130078175\n"
     ]
    }
   ],
   "source": [
    "# Transform X_test using tfidf_vectorizer\n",
    "X_test_svm_task1 = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Run the model against the test set\n",
    "svm_task1_pred = svm.predict(X_test_svm_task1)\n",
    "\n",
    "# Compare the predictions with the actual outcomes; print a confusion matrix\n",
    "svm_task1_results = confusion_matrix(y_test, svm_task1_pred)\n",
    "\n",
    "svm_task1_matrix = []\n",
    "for i in range(0, 5):\n",
    "        svm_task1_matrix.append(svm_task1_results[i,i])\n",
    " \n",
    "print(svm_task1_results)\n",
    "print('The model has a {} percent accuracy score'.format(np.round(np.sum(svm_task1_matrix)/len(X_test) * 100), 3))\n",
    "print(\"The model has a precision score of {}\".format(precision_score(y_test, svm_task1_pred, average='weighted')))\n",
    "print(\"The model has a recall score of {}\".format(recall_score(y_test, svm_task1_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this model does a better job identifying the sentiment correctly, especcially the very negative and very positive reviews. Let's compare the ten most positive and negative words with those of the Multinomial Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top ten most negative words: \n",
      "[(2.159152060488495, 'insignificance'), (2.1743451558033158, 'kills'), (2.1931265090210226, 'turd'), (2.2012723848818396, 'unwatch'), (2.2241261223823208, 'repulsive'), (2.2894162987291153, 'stinks'), (2.2922790258349983, 'disappointment'), (2.2944575677619246, 'puddle'), (2.4196833814226357, 'stinker'), (2.477082732539301, 'awfulness')]\n",
      "\n",
      " The top ten most positive words: \n",
      "[(2.1369176857495886, 'standout'), (2.1585644244037265, 'rivet'), (2.208045800356681, 'enthrall'), (2.229379219828903, 'tremendous'), (2.2317849951722732, 'majestic'), (2.2701573831943125, 'masterful'), (2.306380816767369, 'astoundingly'), (2.3104892741875984, 'terrific'), (2.509939336621655, 'refreshes'), (2.591685388063509, 'zings')]\n"
     ]
    }
   ],
   "source": [
    "# Print top 10 very positive and very negative words.\n",
    "svm_task1_neg = sorted(zip(svm.coef_[0], tfidf_vectorizer.get_feature_names()))\n",
    "svm_task1_very_negative_features = svm_task1_neg[-10:]\n",
    "print(\"The top ten most negative words: \")\n",
    "print(svm_task1_very_negative_features)\n",
    "\n",
    "svm_task1_pos = sorted(zip(svm.coef_[4], tfidf_vectorizer.get_feature_names()))\n",
    "svm_task1_very_positive_features = svm_task1_pos[-10:]\n",
    "print(\"\\r\\n The top ten most positive words: \")\n",
    "print(svm_task1_very_positive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the words selected by the Multionmial Naive Bayes model, they don't make much sense. Words like plot, movie, and film are associated negatively, along with bad and long. The SVM, however, identifies inignificance, dissapointment, stinker, and turd as negative words related to poor movie reviews.  \n",
    "Regarding positive reviews, the MNB picks out words like simple, best, and good; while the SVM identifies more positive words as standout, enthrall, or majestic; which makes more sense and shows that the SVM indeed does a superior job when discriminating between positive and negative reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Task 1 showed that, despite having similar accuracy scores (58 percent vs 62 percent), the SVM does a better job classifying sentiment because the list of words used to descriminate sentiment stand out much more than those chosen by the MNB model. \n",
    "Building on these models, we'll use a Count Vectorizer and both unigram and bigrams to improve both models' accuracy scores.\n",
    "\n",
    "##### Multinomial Naive Bayes\n",
    "\n",
    "We'll start by building the Count Vectorizer, then fitting both the test and training sets to the vectorizer, and creating the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the Count Vectorizer using both uni- and bigram.\n",
    "cv_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1, 2), min_df=2, stop_words='english')\n",
    "\n",
    "# Initiate the Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Fit transform the training set with the count vectorizer\n",
    "X_train_mnb_task2 = cv_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Fit the model on the data \n",
    "mnb.fit(X_train_mnb_task2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  916  1315   530    54    10]\n",
      " [  868  4692  4825   494    46]\n",
      " [  309  2897 24715  3350   326]\n",
      " [   44   493  5154  6510  1118]\n",
      " [    4    52   592  1849  1261]]\n",
      "The model has a 61.0 percent accuracy score\n",
      "The model has a precision score of 0.5963672059380526\n",
      "The model has a recall score of 0.6102460592079969\n"
     ]
    }
   ],
   "source": [
    "# Transform X_test using cv_vectorizer\n",
    "X_test_mnb_task2 = cv_vectorizer.transform(X_test)\n",
    "\n",
    "# Run the model against the test set\n",
    "mnb_task2_pred = mnb.predict(X_test_mnb_task2)\n",
    "\n",
    "# Compare the predictions with the actual outcomes; print a confusion matrix\n",
    "mnb_task2_results = confusion_matrix(y_test, mnb_task2_pred)\n",
    "\n",
    "mnb_task2_matrix = []\n",
    "for i in range(0, 5):\n",
    "        mnb_task2_matrix.append(mnb_task2_results[i,i])\n",
    " \n",
    "print(mnb_task2_results)\n",
    "print('The model has a {} percent accuracy score'.format(np.round(np.sum(mnb_task2_matrix)/len(X_test) * 100), 3))\n",
    "print(\"The model has a precision score of {}\".format(precision_score(y_test, mnb_task2_pred, average='weighted')))\n",
    "print(\"The model has a recall score of {}\".format(recall_score(y_test, mnb_task2_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top ten most negative words: \n",
      "[(-7.151599930249553, 'plot'), (-7.151599930249553, 'worst'), (-7.140426629651428, 'does'), (-7.129376793464843, 'characters'), (-7.06552532147831, 'movi'), (-6.757434377395322, 'just'), (-6.351969269287157, 'like'), (-5.998329229043579, 'bad'), (-5.933126035232817, 'film'), (-5.684398930516862, 'movie')]\n",
      "\n",
      " The top ten most positive words: \n",
      "[(-6.992071807890141, 'comedy'), (-6.992071807890141, 'fun'), (-6.874288772233757, 'work'), (-6.866383592726644, 'great'), (-6.812730879234324, 'year'), (-6.5010845263438135, 'funny'), (-6.468823664125592, 'good'), (-6.234107127253724, 'best'), (-6.169307134026809, 'movie'), (-5.332144495533648, 'film')]\n"
     ]
    }
   ],
   "source": [
    "# Print top 10 very positive and very negative words.\n",
    "mnb_task2_neg = sorted(zip(mnb.coef_[0], cv_vectorizer.get_feature_names()))\n",
    "mnb_task2_very_negative_features = mnb_task2_neg[-10:]\n",
    "print(\"The top ten most negative words: \")\n",
    "print(mnb_task2_very_negative_features)\n",
    "\n",
    "mnb_task2_pos = sorted(zip(mnb.coef_[4], cv_vectorizer.get_feature_names()))\n",
    "mnb_task2_very_positive_features = mnb_task2_pos[-10:]\n",
    "print(\"\\r\\n The top ten most positive words: \")\n",
    "print(mnb_task2_very_positive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new model has a better score, with an overall accuracy of 61 percent. Similarly, both the precision and recall improve from 57.7 and 58.4 percent to 59.6 and 61.0 percent. We can see that the model perfroms better, with most of the labels being distributed more accurately regarding the predicted labels. \n",
    "We'll now build a new SVM. \n",
    "\n",
    "##### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate the SVM\n",
    "svm = LinearSVC()\n",
    "\n",
    "# Fit transform the training set with the count vectorizer\n",
    "X_train_svm_task2 = cv_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Fit the model on the data \n",
    "svm.fit(X_train_svm_task2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  981  1344   431    58    11]\n",
      " [  980  4846  4605   456    38]\n",
      " [  263  2716 25554  2894   170]\n",
      " [   33   366  5098  6407  1415]\n",
      " [    8    34   433  1775  1508]]\n",
      "The model has a 63.0 percent accuracy score\n",
      "The model has a precision score of 0.6156692667969537\n",
      "The model has a recall score of 0.6295014737921313\n"
     ]
    }
   ],
   "source": [
    "# Transform X_test using count_vectorizer\n",
    "X_test_svm_task2 = cv_vectorizer.transform(X_test)\n",
    "\n",
    "# Run the model against the test set\n",
    "svm_task2_pred = svm.predict(X_test_svm_task2)\n",
    "\n",
    "# Compare the predictions with the actual outcomes; print a confusion matrix\n",
    "svm_task2_results = confusion_matrix(y_test, svm_task2_pred)\n",
    "\n",
    "svm_task2_matrix = []\n",
    "for i in range(0, 5):\n",
    "        svm_task2_matrix.append(svm_task2_results[i,i])\n",
    " \n",
    "print(svm_task2_results)\n",
    "print('The model has a {} percent accuracy score'.format(np.round(np.sum(svm_task2_matrix)/len(X_test) * 100), 3))\n",
    "print(\"The model has a precision score of {}\".format(precision_score(y_test, svm_task2_pred, average='weighted')))\n",
    "print(\"The model has a recall score of {}\".format(recall_score(y_test, svm_task2_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also improves upon the previous SVM but not at the same rate that the MNB improved between tasks. The first SVM had an accuracy score of 62 percent, while this second model has an accuracy score of 63 percent. \n",
    "Similarly, both the precision and recall scores improve but again, this improvement is marginal at best. \n",
    "Let's check the top ten positive and negative words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top ten most negative words: \n",
      "[(1.6174420625040353, 'zzzzzzzzz'), (1.624393411570868, 'hated'), (1.649875858250587, 'lam'), (1.6597472124616677, 'stinks'), (1.6718817551537646, 'snoozer'), (1.7652745846712279, 'dread'), (1.7753912565100816, 'baaaaaaaaad'), (1.8425306482789476, 'unbear'), (1.894373281044505, 'grotesqu'), (1.9648968974993428, 'unwatch')]\n",
      "\n",
      " The top ten most positive words: \n",
      "[(1.6005508300412497, 'phenomenal'), (1.607459722815741, 'masterful'), (1.6210873328459603, 'splendid'), (1.626030518362409, 'superb'), (1.6483348401730091, 'masterpiec'), (1.7326633755420633, 'topnotch'), (1.7485043562142197, 'dreamy'), (1.7612453450823295, 'rivet'), (1.772829856378459, 'terrif'), (1.9672066529713204, 'seat tense')]\n"
     ]
    }
   ],
   "source": [
    "# Print top 10 very positive and very negative words.\n",
    "svm_task2_neg = sorted(zip(svm.coef_[0], cv_vectorizer.get_feature_names()))\n",
    "svm_task2_very_negative_features = svm_task2_neg[-10:]\n",
    "print(\"The top ten most negative words: \")\n",
    "print(svm_task2_very_negative_features)\n",
    "\n",
    "svm_task2_pos = sorted(zip(svm.coef_[4], cv_vectorizer.get_feature_names()))\n",
    "svm_task2_very_positive_features = svm_task2_pos[-10:]\n",
    "print(\"\\r\\n The top ten most positive words: \")\n",
    "print(svm_task2_very_positive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, when looking at the words, it is obvious that the support vector machine does a better job at identifying words with both positive and negative connotations than the multinomial Naive Bayes model does. Words like 'zzzzzzz', 'hated', and 'grotesque' are more negative than words like 'bad', 'worst', and 'plot'. The same thing can be said about the positive words, thought the MNB does identify positive words more easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 \n",
    "\n",
    "Now that we've worked on both tasks, we'll train a SVM using the entire training set. We'll then load the real test data from the Kaggle competition, export those results, and submit them to Kaggle. However, we'll have to process the test data in the same way that the training set was porcessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test data set\n",
    "test = pd.read_csv('C:/Users/malon/Documents/Syracuse University/IST 736 Text Mining/IST736/IST736/Week6/kaggle-sentiment/kaggle-sentiment/test.tsv', delimiter='\\t')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test set, removing special characters, lowering the type case, and passing through the stemmer. \n",
    "dat_test = test\n",
    "\n",
    "# Remove special character, set the dataset to lower case\n",
    "for i in range(0, len(dat_test)): \n",
    "    dat_test.loc[i, 'Phrase'] = re.sub('[\\W\\_]', ' ', dat_test.loc[i, 'Phrase']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Phrase  \\\n",
      "0  an intermittently pleasing but mostly routine ...   \n",
      "1  an intermittently pleasing but mostly routine ...   \n",
      "2                                                 an   \n",
      "3  intermittently pleasing but mostly routine effort   \n",
      "4         intermittently pleasing but mostly routine   \n",
      "\n",
      "                                      stemmed_review  \n",
      "0  an intermittently pleasing but mostly routine ...  \n",
      "1  an intermittently pleasing but mostly routine ...  \n",
      "2                                                 an  \n",
      "3  intermittently pleasing but mostly routine effort  \n",
      "4          intermittently pleasing but mostly routin  \n"
     ]
    }
   ],
   "source": [
    "# Initiate English stemmer for testing set\n",
    "englishStemmer=SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "for i in range(0, len(dat_test)): \n",
    "    dat_test.loc[i, 'stemmed_review'] = englishStemmer.stem(dat_test.loc[i, 'Phrase'])\n",
    "    \n",
    "# Review first five observations. \n",
    "print(dat_test[['Phrase', 'stemmed_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the testing data set has been cleaned, we'll ste the training variable and label, along with the test variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the train and test sets\n",
    "X_train = dat['stemmed_review'].values\n",
    "y_train = dat['Sentiment'].values\n",
    "X_test = dat_test['stemmed_review'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of it's superior accuracy rate, we'll keep the count vectorizer. However, this time we'll do some additional parameter tuning, setting the penalty rate to 0.5, initiating a random state of 42, and do 5-fold cross validation to get the best possible accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Count Vectorizer using both uni- and bigram.\n",
    "cv_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1, 2), min_df=2, stop_words='english')\n",
    "\n",
    "# Initiate the model.\n",
    "svm = LinearSVC(C=0.5, random_state=42)\n",
    "\n",
    "# Fit transform X_train and X_test\n",
    "task3_train = cv_vectorizer.fit_transform(X_train)\n",
    "task3_test = cv_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58000961, 0.55574422, 0.54698664, 0.55751362, 0.56706184])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate the model \n",
    "model = cross_val_score(svm, task3_train, y_train, cv=5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8249327181853133"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model \n",
    "svm.fit(task3_train, y_train)\n",
    "\n",
    "# Check the model score\n",
    "svm.score(task3_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the cross validate score having an average score of 56.1 percent, without cross validation, the model has a predicted score of 82.5 percent on the training set. The model will now be used to predict scores for the transformed test set; this will then be submitted to the Kaggle competition for a final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment scores and print first five values\n",
    "pred_scores = svm.predict(task3_test)\n",
    "print(pred_scores[:5])\n",
    "\n",
    "# Transform the resulting array into a one-column data frame, merge with the phraseId column of the test set, and export it to a csv file\n",
    "transformed_scores = pd.DataFrame(pred_scores.reshape(-1,1))\n",
    "score_id = dat_test['PhraseId']\n",
    "\n",
    "final_scores = pd.concat([score_id, transformed_scores], ignore_index=True, axis=1)\n",
    "final_scores.columns = ['PhraseId', 'Sentiment']\n",
    "\n",
    "final_scores.to_csv('Kaggle Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is additional parameter tuning that can be done to improve both the Multinomial Naive Bayes and Support Vector Machine models. Though they have given promising results, improving them requires more careful calibration of their parameters, as any simple parameter tuning might not increase model accuracy sufficiently. \n",
    "\n",
    "As to the Kaggle competition, the submitted model preduced a score of 0.60645, which is not bad for a first attempt but far from the top scores in the competition, which currently sit at around 0.70. If the model were to add lemmitization, as well as analysis of other stop word trends, there is no doubt that the model could be further improved upon. For the time being, this model produces an acceptable ground to build upon. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
